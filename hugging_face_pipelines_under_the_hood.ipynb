{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this tutorial, you will perform three NLP tasks using Hugging Face tokenizers, models, and pipelines. The goal is to learn:\n",
        "\n",
        "- How to use Hugging Face tokenizers for preprocessing tasks like padding, truncation, and batching of text.\n",
        "\n",
        "- How model configuration works, including mapping between id2label and label2id for token classification tasks.\n",
        "\n",
        "- How Hugging Face models work, including passing text through the model to generate logits.\n",
        "\n",
        "- How to use the logits output from Hugging Face models to make predictions for your NLP task.\n",
        "\n",
        "- How to recreate Hugging Face pipelines by using the tokenizers and models directly, instead of relying on the pipelines.\n",
        "\n",
        "- Compare the results of using the tokenizers and models directly versus using the Hugging Face pipelines to evaluate the differences.\n",
        "\n",
        "The focus of this tutorial is gaining hands-on experience with Hugging Face tokenizers, configuration, models, and pipelines through implementing three text processing tasks end-to-end. This will provide a deeper understanding of how these key NLP components work."
      ],
      "metadata": {
        "id": "SL9Sl3TlF74C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Core NLP Libraries\n",
        "\n",
        "This section installs 3 key libraries for NLP and ML projects:\n",
        "\n",
        "- Transformers - Provides access to pretrained models like BERT, RoBERTa for NLP.\n",
        "\n",
        "- Datasets - Provides convenient access to common NLP datasets.\n",
        "\n",
        "- Rich - For nicely formatted console output when training models.\n",
        "\n",
        "Installing these libraries in one line allows quick setup of the Python environment with critical functionality for working on text data.\n"
      ],
      "metadata": {
        "id": "yMR3yJ5bFB6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets rich"
      ],
      "metadata": {
        "id": "VvRBEfGwG3xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformers.pipeline() method provides quick access to pretrained NLP models for making predictions. The rich.pretty.pprint() method prints Python objects to the console in a readable formatted way."
      ],
      "metadata": {
        "id": "ShprEl7DISw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from rich.pretty import pprint\n",
        "import torch.nn.functional as F\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZMc0V7uoG6Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is documentation for each section of the notebook:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Summary\n",
        "\n",
        "By walking through the pipeline components, this shows how to go from raw text to formatted predictions step-by-step. This provides more visibility than just using the packaged pipeline."
      ],
      "metadata": {
        "id": "rbMpxGxuMU_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Sp8xNYwuMsQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Text Classification Pipeline\n",
        "\n",
        "This section creates a text classification pipeline using Hugging Face's transformers library. The pipeline gives quick access to a pretrained DistilBERT model finetuned on the SST-2 sentiment analysis dataset.\n",
        "\n",
        "The pipeline makes predictions on some sample text, returning the sentiment label and score for each sentence."
      ],
      "metadata": {
        "id": "EiQDjpkwMxOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classification = pipeline(task=\"text-classification\")"
      ],
      "metadata": {
        "id": "Xcag9ZH0HY0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "results = classification(raw_inputs)\n",
        "pprint(results)"
      ],
      "metadata": {
        "id": "5v_R0IjAHzFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rOW8wjqjM4Sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Tokenizer, Config, and Model\n",
        "\n",
        "This section loads the lower-level components used by the pipeline:\n",
        "\n",
        "- Tokenizer: Preprocesses the text into ids, handles padding/truncation.\n",
        "\n",
        "- Config: Contains model configuration like hyperparams and mapping from ids to labels.\n",
        "\n",
        "- Model: The core Transformer model like DistilBERT that generates embeddings and predictions.\n",
        "\n",
        "Loading these separately gives more control than just using the packaged pipeline.\n"
      ],
      "metadata": {
        "id": "VAeGzjLQM88m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "Xp8gwNjII7Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
      ],
      "metadata": {
        "id": "cCPuumasH7yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "config = AutoConfig.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "bPKWeLD6I-GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Tokenizing the Text\n",
        "\n",
        "The tokenizer is used to preprocess the raw text into tokenized ids with padding & truncation to fit the expected model input shape.\n",
        "\n",
        "This shows how the tokenizer prepares the data before passing it to the model.\n"
      ],
      "metadata": {
        "id": "Ts_jL7RVNGU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "pprint(inputs)"
      ],
      "metadata": {
        "id": "rXOtZO2HJAwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Printing Truncated Text\n",
        "\n",
        "The truncated input text is decoded back to readable text using the tokenizer's decode method.\n",
        "\n",
        "This shows how padding and truncation end up masking part of the original input.\n",
        "\n",
        "# Passing Inputs to Model\n",
        "\n",
        "The tokenized & padded inputs are passed to the model to generate predictions.\n",
        "\n",
        "This uses the model directly instead of the pipeline, giving more control.\n"
      ],
      "metadata": {
        "id": "8uPhUVnqNN0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(tokenizer.decode(inputs[\"input_ids\"][1]))"
      ],
      "metadata": {
        "id": "FZiAHRRFJOjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpreting Model Outputs\n",
        "\n",
        "The raw numeric tensor outputs of the model are converted into probability scores and sentiment labels.\n",
        "\n",
        "This uses the mapping in the config to go from indices predicted by the model back to the associated labels.\n",
        "\n",
        "The end result matches what the pipeline originally produced.\n"
      ],
      "metadata": {
        "id": "OD8BLmSdNS9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "pprint(outputs)"
      ],
      "metadata": {
        "id": "UIzPgyUaJVHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Covert logits to probabilities"
      ],
      "metadata": {
        "id": "UMiBV9N5NXS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = F.softmax(outputs.logits, dim=-1)\n",
        "pprint(predictions)"
      ],
      "metadata": {
        "id": "AkPIG6J8Jfd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loop through probabilities and convert to interpretable results"
      ],
      "metadata": {
        "id": "D5axmOUCNdyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for index, prediction in enumerate(predictions):\n",
        "  probability = torch.max(prediction).item()\n",
        "  sentiment = config.id2label[torch.argmax(prediction).item()]\n",
        "  result.append({\"probability\": probability, \"label\": sentiment})\n",
        "pprint(result)"
      ],
      "metadata": {
        "id": "439dHeOmKFjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NFkVtShVRIdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Token Classification Pipeline\n",
        "\n",
        "This section creates a named entity recognition (NER) pipeline using the Hugging Face transformers library. The pipeline provides quick access to a pretrained BERT model finetuned on the CoNLL 2003 NER dataset.\n",
        "\n",
        "The pipeline makes predictions on a sample input text, returning the predicted NER tags with scores for each token.\n"
      ],
      "metadata": {
        "id": "DIZ2uXcwROrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_classifier = pipeline(\"token-classification\")"
      ],
      "metadata": {
        "id": "xtfduFt8KNsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting the Pipeline Output\n",
        "\n",
        "The raw JSON output from the NER pipeline is printed to inspect the predicted entity, score, index, word, start and end values for each tagged token."
      ],
      "metadata": {
        "id": "41KiWT4zaQtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_raw_inputs = \"My name is Wolfgang and I live in Berlin\"\n",
        "result = token_classifier(ner_raw_inputs)\n",
        "pprint(result)"
      ],
      "metadata": {
        "id": "_pT5v6bURTwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Pipeline Components\n",
        "\n",
        "The lower level tokenizer, config, and model objects that compose the pipeline are loaded. This gives more control than just using the packaged pipeline.\n"
      ],
      "metadata": {
        "id": "leUltJ7NaVvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "ner_tokenizer = AutoTokenizer.from_pretrained(ner_checkpoint)\n",
        "pprint(ner_tokenizer)"
      ],
      "metadata": {
        "id": "HWblGxnlReEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing the Input\n",
        "\n",
        "The tokenizer preprocesses the raw text into tokenized ids, padding & truncating as needed to match the expected model input shape.\n"
      ],
      "metadata": {
        "id": "unMzfgTead_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model_inputs = ner_tokenizer(ner_raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "2eC9_r9ySFC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passing Inputs to the Model\n",
        "\n",
        "The tokenized inputs are passed to the model to generate predictions. This uses the model directly instead of relying on the pipeline abstraction.\n"
      ],
      "metadata": {
        "id": "6j0nBPH_aiab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(ner_checkpoint)"
      ],
      "metadata": {
        "id": "6n2rRBU1SW_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model_outputs = ner_model(**ner_model_inputs)\n",
        "pprint(ner_model_outputs)"
      ],
      "metadata": {
        "id": "W1ZBFv4hSbxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpreting Model Outputs\n",
        "\n",
        "The raw tensor outputs are converted to probability scores over the possible entity tags for each token.\n"
      ],
      "metadata": {
        "id": "HcM6-5fqamNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_predictions = F.softmax(ner_model_outputs.logits, dim=-1)\n",
        "pprint(ner_predictions)"
      ],
      "metadata": {
        "id": "I_6RkXDkSk5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting to Human-Readable Outputs\n",
        "\n",
        "The probabilities are parsed to extract the highest scoring entity tag per token. The start and end offsets are looked up based on the original input text.\n",
        "\n",
        "This mirrors the output format returned by the pipeline to extract human-readable entity, score, start, end results.\n"
      ],
      "metadata": {
        "id": "RhnLNhd9aqso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_results= []\n",
        "for index, prediction in enumerate(ner_predictions[0]):\n",
        "  prediction_probability = torch.max(prediction).item()\n",
        "  prediction_id = torch.argmax(prediction).item()\n",
        "  if prediction_id > 0:\n",
        "    entity = ner_model.config.id2label[prediction_id]\n",
        "    word = ner_tokenizer.decode(ner_model_inputs.input_ids[0][index])\n",
        "    start = ner_raw_inputs.find(word)\n",
        "    end = start + len(word)\n",
        "    ner_results.append({\"entity\":entity,\"score\":prediction_probability, \"index\": index, \"word\": word, \"start\": start, \"end\": end})\n",
        "pprint(ner_results)"
      ],
      "metadata": {
        "id": "YEs2OO5DS5xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load QA Model\n",
        "\n",
        "- qa_checkpoint: Specifies pretrained QA model from Hugging Face Hub to use\n",
        "\n",
        "- pipeline: Constructs question answering pipeline object using the QA model\n",
        "\n",
        "# Define Question and Context\n",
        "\n",
        "- question: Question text string to ask the model\n",
        "\n",
        "- context: Context paragraphs providing information to answer question\n",
        "\n",
        "# Get QA Predictions\n",
        "\n",
        "- qa_pipeline: Runs input question and context through model to make predictions\n",
        "\n",
        "- qa_results: Contains predicted answer text and confidence score\n",
        "\n",
        "- pprint: Prints prediction results in readable formatted output\n",
        "\n",
        "This code loads a pretrained QA model, defines a question and context, passes them through the pipeline to generate an answer prediction, and prints the prediction nicely formatted. The pipeline handles running the inputs through the full model to output the top answer text span and score."
      ],
      "metadata": {
        "id": "aVah-hs3r-e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_checkpoint = \"deepset/roberta-base-squad2\"\n",
        "qa_pipeline = pipeline(\"question-answering\",model=qa_checkpoint)\n",
        "\n",
        "question = \"What is the capital of France?\"\n",
        "context = \"The capital of France is Paris.\"\n",
        "\n",
        "qa_results = qa_pipeline(question,context)\n",
        "pprint(qa_results)"
      ],
      "metadata": {
        "id": "49J6LAnAdcs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads pretrained question answering model using AutoModelForQuestionAnswering class.\n",
        "\n",
        "Loads corresponding tokenizer using AutoTokenizer that was used during model training.\n",
        "\n",
        "Tokenizer preprocesses text to numeric ids.\n",
        "\n",
        "Model generates start and end logits to predict answer span."
      ],
      "metadata": {
        "id": "bVm6T12gsvlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_checkpoint)\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(qa_checkpoint)"
      ],
      "metadata": {
        "id": "Y2lakssMbQk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "qa_model_inputs = qa_tokenizer(question, context, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "pprint(qa_model_inputs)"
      ],
      "metadata": {
        "id": "kHNud9vZcCfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model_outputs = qa_model(**qa_model_inputs)\n",
        "pprint(qa_model_outputs)"
      ],
      "metadata": {
        "id": "7bzHR-bHcQ3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is brief documentation for the provided code snippet:\n",
        "\n",
        "# Extract Logits\n",
        "\n",
        "- Get start and end logits from model outputs\n",
        "\n",
        "# Get Prediction Indices\n",
        "\n",
        "- Find index of maximum start and end logits\n",
        "\n",
        "# Decode Answer Text\n",
        "\n",
        "- Extract predicted answer tokens from input ids\n",
        "\n",
        "- Convert tokens back to text with tokenizer\n",
        "\n",
        "# Compute Probability\n",
        "\n",
        "- Take softmax of start and end logits\n",
        "\n",
        "- Find max joint probability of start and end\n",
        "\n",
        "# Format Human-Readable Output\n",
        "\n",
        "- Get start and end char offsets in context\n",
        "\n",
        "- Format into dict with score, text, offsets\n",
        "\n",
        "# Print Output\n",
        "\n",
        "- Display prediction result nicely formatted\n",
        "\n",
        "This takes the raw start and end logits from the model, picks the most likely start and end points, extracts the predicted answer text, computes the overall probability, and formats into a human-readable output with score and answer text."
      ],
      "metadata": {
        "id": "3A5H_FFpu8E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_logits, end_logits = qa_model_outputs.start_logits, qa_model_outputs.end_logits"
      ],
      "metadata": {
        "id": "fBVdlt0lceVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_pos = torch.argmax(start_logits, dim = -1)\n",
        "end_pos = torch.argmax(end_logits, dim = -1) + 1\n",
        "pprint((start_pos,end_pos))"
      ],
      "metadata": {
        "id": "yaVqcAlDcpk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_tokens = qa_model_inputs.input_ids[0][start_pos: end_pos]\n",
        "answer = qa_tokenizer.decode(answer_tokens)\n",
        "start_probs, end_probs = F.softmax(qa_model_outputs.start_logits,dim=-1), F.softmax(qa_model_outputs.end_logits,dim=-1)\n",
        "probability = torch.max(start_probs*end_probs).item()\n",
        "start = context.find(answer)\n",
        "end = start + len(answer)\n",
        "result = {\"score\": probability, \"answer\": answer, \"start\": start, \"end\": end}\n",
        "pprint(result)"
      ],
      "metadata": {
        "id": "4swu5O59cwGe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}